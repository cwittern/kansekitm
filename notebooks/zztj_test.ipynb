{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# 資治通鑑 唐記 topic modeling\n",
    "\n",
    "This notebook loads a corpus of the 資治通鑑 唐記 to experiment with different settings for topic modeling.  The data have been created in the project \"Toward an Overall Inheritance and Development of Kanji Culture, East Asian Center for Informatics in Humanities, the 21st Century COE, Kyoto University\" (http://coe21.zinbun.kyoto-u.ac.jp/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "stopwords=[\n",
    "# u'一月', u'正月', u'二月', u'三月', u'四月', u'五月', u'六月',\n",
    "# u'七月', u'八月', u'九月', u'十月', u'十一月', u'十二月',\n",
    "u'之', u'是', u'于', u'元', u'哉', u'還', u'甚', u'氏', u'焉', u'不', u'與',\n",
    "u'在', u'外', u'也', u'夫', u'非', u'稱', u'左', u'以', u'可', u'六',\n",
    "u'雖', u'屬', u'己', u'興', u'千', u'而', u'五', u'諸', u'足', u'邪',\n",
    "u'耳', u'亦', u'其', u'將', u'又', u'九', u'然', u'高', u'終', u'首',\n",
    "u'益', u'通', u'常', u'七', u'曰', u'何', u'若', u'內', u'女', u'遠',\n",
    "u'由', u'應', u'對', u'過', u'方', u'者', u'至', u'及', u'當', u'右',\n",
    "u'盡', u'共', u'十', u'所', u'此', u'乃', u'子', u'四', u'未', u'去',\n",
    "u'敢', u'異', u'徒', u'則', u'故', u'太', u'百', u'有', u'矣', u'萬',\n",
    "u'北', u'前', u'進', u'任', u'無', u'三', u'謂', u'皆', u'於', u'自',\n",
    "u'吾', u'來', u'易', u'初', u'更', u'一', u'二', u'如', u'乎',\n",
    "#u'甲子', u'乙丑', u'丙寅', u'丁卯', u'戊辰', u'己巳', u'庚午', u'辛未', u'壬申',\n",
    "#u'癸酉', u'甲戌', u'乙亥', u'丙子', u'丁丑', u'戊寅', u'己卯', u'庚辰',\n",
    "#u'辛巳', u'壬午', u'癸未', u'甲申', u'乙酉', u'丙戌', u'丁亥', u'戊子',\n",
    "#u'己丑', u'庚寅', u'辛卯', u'壬辰', u'癸巳', u'甲午', u'乙未', u'丙申',\n",
    "#u'丁酉', u'戊戌', u'己亥', u'庚子', u'辛丑', u'壬寅', u'癸卯', u'甲辰',\n",
    "#u'乙巳', u'丙午', u'丁未', u'戊申', u'己酉', u'庚戌', u'辛亥', u'壬子',\n",
    "#u'癸丑', u'甲寅', u'乙卯', u'丙辰', u'丁巳', u'戊午', u'己未', u'庚申',\n",
    "#u'辛酉', u'壬戌', u'癸亥',\n",
    "]\n",
    "print \",\".join(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The following codes prepares for loading the corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import gensim, mmseg, os, codecs, re\n",
    "from collections import defaultdict\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "punc_re = re.compile(ur\"[\\u3001-\\u33FF\\uFF00-\\uFF7F]\")\n",
    "\n",
    "def mmseg_split(s, normalize_ent=False):\n",
    "    \"Split using mmseq only if not an identified named entity.\"\n",
    "    out = []\n",
    "    for l in s.split():\n",
    "        if \"@\" in l:\n",
    "            out.append(l.split('@')[-1])\n",
    "        else:\n",
    "            algor = mmseg.Algorithm(l)\n",
    "            out.extend([tok.text for tok in algor if not (punc_re.search(tok.text) or tok.text in stopwords)])\n",
    "    # tokenize only on kanji:\n",
    "    # out = \" \".join([a for a in re.split(\"(.)\", out) if len(a) > 0])\n",
    "    return out\n",
    "\n",
    "doclabel = []\n",
    "def load_documents(files):\n",
    "    ret = []\n",
    "    for f in files:\n",
    "        r1 = []\n",
    "        pcnt = 0\n",
    "        for line in codecs.open(f, 'r', 'utf-8'):\n",
    "            if line[0] in ['*', '#']:\n",
    "                if ' year' in line:\n",
    "                    y = line[:-1].split()[-1]\n",
    "                    pcnt = 0\n",
    "                elif ' p' in line:\n",
    "                    pcnt += 1\n",
    "                    ret.append(r1)\n",
    "                    doclabel.append(\"%s-%2.2d\" % (y, pcnt))\n",
    "                    r1 = []\n",
    "                continue\n",
    "            r1.extend(mmseg_split(line))\n",
    "        ret.append(r1)\n",
    "    return ret\n",
    "\n",
    "kstm_base='/home/chris/00scratch/kansekitm'\n",
    "corpus_base='%s/corpus/zztj' % (kstm_base)\n",
    "t = mmseg.Dictionary()\n",
    "t.load_chars('%s/dic/chars.dic' % (kstm_base ))\n",
    "t.load_words('%s/dic/words.dic' % (kstm_base ))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Now we are loading the corpus and produce the frequency table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# stopwords=[]\n",
    "print \"Number of stopwords: %d\" % (len(stopwords))\n",
    "cut_off = 20\n",
    "files = os.listdir(corpus_base)\n",
    "files.sort()\n",
    "files = [\"%s/%s\" % (corpus_base, f) for f in files if f.startswith('zztj')]\n",
    "texts = load_documents(files)\n",
    "print \"Loaded %d documents (paragraphs)\" % (len(texts))\n",
    "\n",
    "frequency=defaultdict(int)\n",
    "for p in texts:\n",
    "    for token in p:\n",
    "        frequency[token] += 1\n",
    "\n",
    "fsum = sum([a for b, a in frequency.iteritems()])\n",
    "csum = sum([len(b)*a for b, a in frequency.iteritems()])\n",
    "print \"Total # of characters: \", csum\n",
    "fq = [a for a in sorted(frequency.iteritems(), key=lambda (k,v): (v,k), reverse=True)]\n",
    "print \"%d most frequent characters:\" % (cut_off)\n",
    "for f in fq[0:cut_off]:\n",
    "    print \"%s\\t%5.5d\\t%2.2f\" % (f[0], f[1], (f[1] / fsum) * 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "doc=10\n",
    "print doclabel[doc]\n",
    "print \" \".join(texts[doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "min_freq = 1\n",
    "max_freq = 5000\n",
    "red_texts = [[token for token in text if frequency[token] > min_freq and frequency[token] < max_freq]\n",
    "              for text in texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(texts)\n",
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "# dictionary.filter_extremes(no_below=20, no_above=0.5)\n",
    "\n",
    "corpus = [dictionary.doc2bow(text) for text in red_texts]\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics = 10\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make a index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "%time model = LdaModel(corpus=corpus, id2word=id2word, chunksize=chunksize, \\\n",
    "                       alpha='auto', eta='auto', \\\n",
    "                       iterations=iterations, num_topics=num_topics, \\\n",
    "                       passes=passes, eval_every=eval_every)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "top_topics=model.top_topics(corpus)\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "for i, t in enumerate(top_topics):\n",
    "    print i, \",\".join([z[1] for z in t[0]])\n",
    "    #print t\n",
    "    #print \",\".join([a[1] for a in t])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "name": "zztj_test.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
